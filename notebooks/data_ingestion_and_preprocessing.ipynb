{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8db71c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: telethon in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (1.40.0)\n",
      "Requirement already satisfied: pyaes in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from telethon) (1.6.1)\n",
      "Requirement already satisfied: rsa in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from telethon) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from rsa->telethon) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: Pillow in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (11.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-magic-bin in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (0.4.14)\n",
      "Requirement already satisfied: numpy in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: telethon in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (1.40.0)\n",
      "Requirement already satisfied: pyaes in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from telethon) (1.6.1)\n",
      "Requirement already satisfied: rsa in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from telethon) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from rsa->telethon) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Fetching messages from @ZemenExpress...\n",
      "Error accessing channel @ZemenExpress: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@ZemenExpress_messages.csv\n",
      "Fetching messages from @nevacomputer...\n",
      "Error accessing channel @nevacomputer: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@nevacomputer_messages.csv\n",
      "Fetching messages from @meneshayeofficial...\n",
      "Error accessing channel @meneshayeofficial: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@meneshayeofficial_messages.csv\n",
      "Fetching messages from @ethio_brand_collection...\n",
      "Error accessing channel @ethio_brand_collection: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@ethio_brand_collection_messages.csv\n",
      "Fetching messages from @Leyueqa@AwasMart...\n",
      "Error accessing channel @Leyueqa@AwasMart: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@Leyueqa@AwasMart_messages.csv\n",
      "Data collection complete!\n",
      "Fetching messages from @ZemenExpress...\n",
      "Error accessing channel @ZemenExpress: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@ZemenExpress_messages.csv\n",
      "Fetching messages from @nevacomputer...\n",
      "Error accessing channel @nevacomputer: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@nevacomputer_messages.csv\n",
      "Fetching messages from @meneshayeofficial...\n",
      "Error accessing channel @meneshayeofficial: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@meneshayeofficial_messages.csv\n",
      "Fetching messages from @ethio_brand_collection...\n",
      "Error accessing channel @ethio_brand_collection: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@ethio_brand_collection_messages.csv\n",
      "Fetching messages from @Leyueqa@AwasMart...\n",
      "Error accessing channel @Leyueqa@AwasMart: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@Leyueqa@AwasMart_messages.csv\n",
      "Data collection complete!\n",
      "Scraping complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_12832\\1011795864.py:43: RuntimeWarning: coroutine 'AuthMethods._start' was never awaited\n",
      "  self.client.start(PHONE_NUMBER)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_12832\\1011795864.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_12832\\1011795864.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_12832\\1011795864.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_12832\\1011795864.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_12832\\1011795864.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_12832\\1011795864.py:43: RuntimeWarning: coroutine 'AuthMethods._start' was never awaited\n",
      "  self.client.start(PHONE_NUMBER)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_12832\\1011795864.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_12832\\1011795864.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_12832\\1011795864.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_12832\\1011795864.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_12832\\1011795864.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to fully_processed_data.csv\n",
      "Data preprocessing complete!\n",
      "Successfully labeled 50 messages in CoNLL format.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [48 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \u001b[35m\"c:\\Users\\Yohanan\\Amharic-E-commerce-data-extractor\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "          \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"c:\\Users\\Yohanan\\Amharic-E-commerce-data-extractor\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "          json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "                                   \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"c:\\Users\\Yohanan\\Amharic-E-commerce-data-extractor\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "          return hook(config_settings)\n",
      "        File \u001b[35m\"C:\\Users\\Yohanan\\AppData\\Local\\Temp\\pip-build-env-_yvugmf2\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "          return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "                 \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yohanan\\AppData\\Local\\Temp\\pip-build-env-_yvugmf2\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "          \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yohanan\\AppData\\Local\\Temp\\pip-build-env-_yvugmf2\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m512\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "          \u001b[31msuper().run_setup\u001b[0m\u001b[1;31m(setup_script=setup_script)\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yohanan\\AppData\\Local\\Temp\\pip-build-env-_yvugmf2\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "          \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "          \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m128\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yohanan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\"\u001b[0m, line \u001b[35m414\u001b[0m, in \u001b[35mcheck_call\u001b[0m\n",
      "          retcode = call(*popenargs, **kwargs)\n",
      "        File \u001b[35m\"C:\\Users\\Yohanan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\"\u001b[0m, line \u001b[35m395\u001b[0m, in \u001b[35mcall\u001b[0m\n",
      "          with \u001b[31mPopen\u001b[0m\u001b[1;31m(*popenargs, **kwargs)\u001b[0m as p:\n",
      "               \u001b[31m~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yohanan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\"\u001b[0m, line \u001b[35m1039\u001b[0m, in \u001b[35m__init__\u001b[0m\n",
      "          \u001b[31mself._execute_child\u001b[0m\u001b[1;31m(args, executable, preexec_fn, close_fds,\u001b[0m\n",
      "          \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "                              \u001b[1;31mpass_fds, cwd, env,\u001b[0m\n",
      "                              \u001b[1;31m^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "          ...<5 lines>...\n",
      "                              \u001b[1;31mgid, gids, uid, umask,\u001b[0m\n",
      "                              \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "                              \u001b[1;31mstart_new_session, process_group)\u001b[0m\n",
      "                              \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        File \u001b[35m\"C:\\Users\\Yohanan\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\"\u001b[0m, line \u001b[35m1551\u001b[0m, in \u001b[35m_execute_child\u001b[0m\n",
      "          hp, ht, pid, tid = \u001b[31m_winapi.CreateProcess\u001b[0m\u001b[1;31m(executable, args,\u001b[0m\n",
      "                             \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "                                   \u001b[1;31m# no special security\u001b[0m\n",
      "                                   \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "          ...<4 lines>...\n",
      "                                   \u001b[1;31mcwd,\u001b[0m\n",
      "                                   \u001b[1;31m^^^^\u001b[0m\n",
      "                                   \u001b[1;31mstartupinfo)\u001b[0m\n",
      "                                   \u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "      \u001b[1;35mFileNotFoundError\u001b[0m: \u001b[35m[WinError 2] The system cannot find the file specified\u001b[0m\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 454\u001b[39m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForTokenClassification\n\u001b[32m    448\u001b[39m \u001b[38;5;66;03m# Use use_fast=False for compatibility with some multilingual models\u001b[39;00m\n\u001b[32m    449\u001b[39m \u001b[38;5;66;03m#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m#model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=5)  # adjust num_labels as needed\u001b[39;00m\n\u001b[32m    451\u001b[39m \n\u001b[32m    452\u001b[39m \u001b[38;5;66;03m# Step 4: Load or Parse CoNLL-format Data\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;66;03m# using Hugging Face `load_dataset`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m model = \u001b[43mAutoModelForTokenClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(model_checkpoint, num_labels=\u001b[32m5\u001b[39m)  \u001b[38;5;66;03m# adjust num_labels as needed\u001b[39;00m\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForTokenClassification\n\u001b[32m    456\u001b[39m \u001b[38;5;66;03m# Load tokenizer\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yohanan\\Amharic-E-commerce-data-extractor\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1885\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   1883\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mis_dummy\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mmro\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1884\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m1885\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yohanan\\Amharic-E-commerce-data-extractor\\.venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1871\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   1868\u001b[39m         failed.append(msg.format(name))\n\u001b[32m   1870\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m1871\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install telethon \n",
    "%pip install Pillow \n",
    "%pip install python-magic-bin numpy\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "%pip install telethon\n",
    "from telethon import TelegramClient\n",
    "from telethon.tl.types import MessageMediaPhoto, MessageMediaDocument\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import magic\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuration\n",
    "API_ID = '20456758'  # Replace with your API ID\n",
    "API_HASH = '83bb06a8c677ed8128784c2dc575aff6'  # Replace with your API hash\n",
    "PHONE_NUMBER = '+251920747086'  # Replace with your phone number\n",
    "SESSION_NAME = 'amharic_ecommerce_scraper'\n",
    "# List of Ethiopian e-commerce channels to monitor\n",
    "CHANNELS = [\n",
    "    '@ZemenExpress',          \n",
    "    '@nevacomputer',    \n",
    "    '@meneshayeofficial',\n",
    "    '@ethio_brand_collection',       \n",
    "    '@Leyueqa' \n",
    "    '@AwasMart',    \n",
    "]\n",
    "# Output directories\n",
    "RAW_DATA_DIR = 'raw_data'\n",
    "PROCESSED_DATA_DIR = 'processed_data'\n",
    "IMAGES_DIR = os.path.join(RAW_DATA_DIR, 'images')\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "class TelegramScraper:\n",
    "    def __init__(self):\n",
    "        self.client = TelegramClient(SESSION_NAME, API_ID, API_HASH)\n",
    "        self.client.start(PHONE_NUMBER)\n",
    "\n",
    "    def fetch_channel_messages(self, channel, limit=100):\n",
    "        \"\"\"\n",
    "        Fetch messages from a Telegram channel.\n",
    "        Returns a list of dictionaries representing messages.\n",
    "        \"\"\"\n",
    "        messages = []\n",
    "        try:\n",
    "            entity = self.client.get_entity(channel)\n",
    "            for message in self.client.iter_messages(entity, limit=limit):\n",
    "                messages.append({\n",
    "                    'channel': channel,\n",
    "                    'message_id': message.id,\n",
    "                    'date': message.date,\n",
    "                    'text': message.text,\n",
    "                    'views': getattr(message, 'views', None),\n",
    "                    'sender_id': getattr(message, 'sender_id', None)\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching messages from {channel}: {e}\")\n",
    "        return messages\n",
    " \n",
    "    def clean_amharic_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean and normalize Amharic text\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "# Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove emojis and special characters (keeping Amharic characters)\n",
    "        # Amharic Unicode range: U+1200 to U+137F\n",
    "        text = re.sub(r'[^\\u1200-\\u137F\\s.,!?።፣፤፥፦፧፨0-9a-zA-Z]', '', text)\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        \"\"\"\n",
    "        Basic entity extraction for Amharic e-commerce text\n",
    "        \"\"\"\n",
    "        entities = {\n",
    "            'prices': [],\n",
    "            'products': [],\n",
    "            'locations': [],\n",
    "            'contacts': []\n",
    "        }\n",
    "        \n",
    "        if not text:\n",
    "            return entities\n",
    "        \n",
    "        # Extract prices (numbers with currency symbols or words)\n",
    "        price_patterns = [\n",
    "            r'(\\d+)\\s*(ብር|br|birr|BR|ብሮ|ብሬ)',  # Ethiopian Birr\n",
    "            r'\\$\\s*(\\d+)',                      # US Dollars\n",
    "            r'(\\d+)\\s*(ዶላር|dollar)'             # Dollars in Amharic\n",
    "        ]\n",
    "        \n",
    "        for pattern in price_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            for match in matches:\n",
    "                if match[0].isdigit():\n",
    "                    entities['prices'].append(match[0] + ' ' + (match[1] if len(match) > 1 else 'ብር'))\n",
    "        \n",
    "        # Extract phone numbers (Ethiopian format)\n",
    "        phone_matches = re.findall(r'(?:\\+251|0)(?:9\\d{8}|[1-9]\\d{7})', text)\n",
    "        entities['contacts'].extend(phone_matches)\n",
    "        \n",
    "        # Extract locations (common Ethiopian cities/areas)\n",
    "        locations = ['አዲስ አበባ', 'ባህር �ር', 'ድሬ ዳዋ', 'ጅማ', 'መቀሌ', \n",
    "                    'አዋሳ', 'አርባ ምንጭ', 'አዲስ አበባ', 'ባህር ዳር', 'ጎንደር']\n",
    "        entities['locations'] = [loc for loc in locations if loc in text]\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def download_media(self, message):\n",
    "        \"\"\"\n",
    "        Download and process media files (images, documents)\n",
    "        \"\"\"\n",
    "        media_info = {\n",
    "            'media_type': None,\n",
    "            'file_path': None,\n",
    "            'file_size': None,\n",
    "            'dimensions': None\n",
    "        }\n",
    "        \n",
    "        if not message.media:\n",
    "            return media_info\n",
    "        \n",
    "        try:\n",
    "            if isinstance(message.media, MessageMediaPhoto):\n",
    "                # Download photo\n",
    "                media_info['media_type'] = 'photo'\n",
    "                file_path = os.path.join(IMAGES_DIR, f'photo_{message.id}.jpg')\n",
    "                self.client.download_media(message.media, file=file_path)\n",
    "                \n",
    "                # Get image dimensions\n",
    "                with Image.open(file_path) as img:\n",
    "                    media_info['dimensions'] = f\"{img.width}x{img.height}\"\n",
    "                \n",
    "                media_info['file_path'] = file_path\n",
    "                media_info['file_size'] = os.path.getsize(file_path)\n",
    "                \n",
    "            elif isinstance(message.media, MessageMediaDocument):\n",
    "                # Download document\n",
    "                media_info['media_type'] = 'document'\n",
    "                file_name = f'document_{message.id}'\n",
    "                file_path = os.path.join(RAW_DATA_DIR, file_name)\n",
    "                self.client.download_media(message.media, file=file_path)\n",
    "                \n",
    "                # Get file type\n",
    "                mime = magic.Magic(mime=True)\n",
    "                file_type = mime.from_file(file_path)\n",
    "                \n",
    "                # Rename file with proper extension\n",
    "                ext = file_type.split('/')[-1]\n",
    "                new_path = f\"{file_path}.{ext}\"\n",
    "                os.rename(file_path, new_path)\n",
    "                \n",
    "                media_info['file_path'] = new_path\n",
    "                media_info['file_size'] = os.path.getsize(new_path)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading media: {e}\")\n",
    "        \n",
    "        return media_info\n",
    "    \n",
    "    def fetch_channel_messages(self, channel_name, limit=1000):\n",
    "        \"\"\"\n",
    "        Fetch messages from a Telegram channel\n",
    "        \"\"\"\n",
    "        print(f\"Fetching messages from {channel_name}...\")\n",
    "        \n",
    "        messages_data = []\n",
    "        try:\n",
    "            channel = self.client.get_entity(channel_name)\n",
    "            \n",
    "            for message in self.client.iter_messages(channel, limit=limit):\n",
    "                try:\n",
    "                    # Basic message info\n",
    "                    msg_data = {\n",
    "                        'channel': channel_name,\n",
    "                        'message_id': message.id,\n",
    "                        'date': message.date,\n",
    "                        'text': self.clean_amharic_text(message.text),\n",
    "                        'views': message.views if hasattr(message, 'views') else None,\n",
    "                        'sender': message.sender_id if hasattr(message, 'sender_id') else None\n",
    "                    }\n",
    "                    \n",
    "                    # Download and process media\n",
    "                    media_info = self.download_media(message)\n",
    "                    msg_data.update(media_info)\n",
    "                    \n",
    "                    # Extract entities from text\n",
    "                    entities = self.extract_entities(message.text)\n",
    "                    msg_data.update(entities)\n",
    "                    \n",
    "                    messages_data.append(msg_data)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing message {message.id}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing channel {channel_name}: {e}\")\n",
    "        \n",
    "        return messages_data\n",
    "    \n",
    "    def save_to_csv(self, data, filename):\n",
    "        \"\"\"Save data to CSV file\"\"\"\n",
    "        df = pd.DataFrame(data)\n",
    "        filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "        df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "        print(f\"Saved {len(df)} records to {filepath}\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Main execution method\"\"\"\n",
    "        all_messages = []\n",
    "        \n",
    "        # Fetch messages from all channels\n",
    "        for channel in CHANNELS:\n",
    "            channel_messages = self.fetch_channel_messages(channel)\n",
    "            all_messages.extend(channel_messages)\n",
    "            \n",
    "            # Save individual channel data\n",
    "            self.save_to_csv(channel_messages, f'{channel}_messages.csv')\n",
    "        \n",
    "        # Save combined data\n",
    "        if all_messages:\n",
    "            self.save_to_csv(all_messages, 'all_channels_combined.csv')\n",
    "        \n",
    "        print(\"Data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = TelegramScraper()\n",
    "    scraper.run()\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load all CSV files from the data directory, skipping empty files\"\"\"\n",
    "        all_data = []\n",
    "        for file in os.listdir(self.data_dir):\n",
    "            if file.endswith('.csv'):\n",
    "                filepath = os.path.join(self.data_dir, file)\n",
    "                try:\n",
    "                    # Check if file is not empty before reading\n",
    "                    if os.path.getsize(filepath) == 0:\n",
    "                        print(f\"Skipped empty file: {filepath}\")\n",
    "                        continue\n",
    "                    df = pd.read_csv(filepath, encoding='utf-8')\n",
    "                    if not df.empty:\n",
    "                        all_data.append(df)\n",
    "                except pd.errors.EmptyDataError:\n",
    "                    print(f\"Skipped empty file: {filepath}\")\n",
    "        \n",
    "        if all_data:\n",
    "            return pd.concat(all_data, ignore_index=True)\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Advanced text preprocessing for Amharic\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove repeated characters (common in informal text)\n",
    "        text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "        \n",
    "        # Normalize Amharic numbers to Western numerals\n",
    "        amharic_numerals = {\n",
    "            '፩': '1', '፪': '2', '፫': '3', '፬': '4', '፭': '5',\n",
    "            '፮': '6', '፯': '7', '፰': '8', '፱': '9', '፲': '10'\n",
    "        }\n",
    "        \n",
    "        for am_num, num in amharic_numerals.items():\n",
    "            text = text.replace(am_num, num)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def process_data(self, df):\n",
    "        \"\"\"\n",
    "        Process the raw dataframe\n",
    "        \"\"\"\n",
    "        # Check if 'text' column exists\n",
    "        if 'text' not in df.columns:\n",
    "            print(\"Warning: 'text' column not found in DataFrame. Returning empty DataFrame.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Clean text\n",
    "        df['clean_text'] = df['text'].apply(self.preprocess_text)\n",
    "        \n",
    "        # Convert date to datetime if 'date' column exists\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        \n",
    "        # Extract additional features\n",
    "        df['text_length'] = df['clean_text'].apply(len)\n",
    "        df['word_count'] = df['clean_text'].apply(lambda x: len(x.split()))\n",
    "        \n",
    "        # Handle missing values for expected columns\n",
    "        for col in ['prices', 'locations', 'contacts']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna('[]')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def save_processed_data(self, df, output_file):\n",
    "        \"\"\"Save processed data to file\"\"\"\n",
    "        df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        print(f\"Processed data saved to {output_file}\")\n",
    "# ...existing code...\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = TelegramScraper()\n",
    "    scraper.run()\n",
    "    print(\"Scraping complete!\")\n",
    "    preprocessor = DataPreprocessor(RAW_DATA_DIR)  # <-- use RAW_DATA_DIR here\n",
    "    raw_data = preprocessor.load_data()\n",
    "    processed_data = preprocessor.process_data(raw_data)\n",
    "    preprocessor.save_processed_data(processed_data, 'fully_processed_data.csv')\n",
    "    print(\"Data preprocessing complete!\")\n",
    "    \n",
    "    import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the dataset (assuming we're using the processed data from Task 1)\n",
    "import os\n",
    "if os.path.exists('fully_processed_data.csv') and os.path.getsize('fully_processed_data.csv') > 0:\n",
    "    df = pd.read_csv('fully_processed_data.csv', encoding='utf-8')\n",
    "else:\n",
    "    print(\"Warning: 'fully_processed_data.csv' is empty or does not exist. Creating empty DataFrame.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# Select 50 random messages for labeling\n",
    "sample_messages = df['text'].dropna().sample(50, random_state=42).tolist()\n",
    "\n",
    "def tokenize_amharic(text):\n",
    "    \"\"\"Tokenize Amharic text (simple whitespace tokenizer with some special handling)\"\"\"\n",
    "    # Split on whitespace but keep common punctuation attached to words\n",
    "    tokens = re.findall(r'\\w+[\\']?\\w*|[\\w\\.-]+|[\\u1200-\\u137F]+|[^\\w\\s]', text)\n",
    "    return [token for token in tokens if token.strip()]\n",
    "\n",
    "def label_message(message):\n",
    "    \"\"\"Label entities in a single message according to CoNLL format\"\"\"\n",
    "    tokens = tokenize_amharic(message)\n",
    "    labels = ['O'] * len(tokens)  # Initialize all tokens as 'O'\n",
    "    \n",
    "    # Patterns for entity recognition\n",
    "    price_pattern = re.compile(r'(\\d+)\\s*(ብር|br|birr|BR|ብሮ|ብሬ|ዶላር|dollar)')\n",
    "    location_words = {'አዲስ አበባ', 'ባህር ዳር', 'ድሬ ዳዋ', 'ጅማ', 'መቀሌ', \n",
    "                     'አዋሳ', 'አርባ ምንጭ', 'ጎንደር', 'ቦሌ', 'ፒያሳ', \n",
    "                     'ሰማንያ', 'አቃቂ', 'ለሚ ካምፓ', 'ሰበር', 'መኮንን'}\n",
    "    \n",
    "    # Convert tokens back to string for pattern matching\n",
    "    text_for_matching = ' '.join(tokens)\n",
    "    \n",
    "    # Label prices\n",
    "    for match in price_pattern.finditer(text_for_matching):\n",
    "        price_start, price_end = match.span()\n",
    "        price_tokens = []\n",
    "        current_pos = 0\n",
    "        for i, token in enumerate(tokens):\n",
    "            token_start = current_pos\n",
    "            token_end = token_start + len(token)\n",
    "            current_pos += len(token) + 1  # +1 for the space\n",
    "            \n",
    "            # Check if token is within the matched price\n",
    "            if not (token_end <= price_start or token_start >= price_end):\n",
    "                price_tokens.append(i)\n",
    "        \n",
    "        if price_tokens:\n",
    "            labels[price_tokens[0]] = 'B-PRICE'\n",
    "            for i in price_tokens[1:]:\n",
    "                labels[i] = 'I-PRICE'\n",
    "    \n",
    "    # Label locations (simple approach)\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in location_words:\n",
    "            labels[i] = 'B-LOC'\n",
    "    \n",
    "    # Label products (this would normally require more sophisticated approach)\n",
    "    # For demo, we'll look for common product indicators\n",
    "    product_keywords = {\n",
    "        'ለገበያ': 'B-PRODUCT',\n",
    "        'ሽያጭ': 'B-PRODUCT', \n",
    "        'ይገኛል': 'B-PRODUCT',\n",
    "        'መሸጥ': 'B-PRODUCT',\n",
    "        'መግዛት': 'B-PRODUCT'\n",
    "    }\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in product_keywords:\n",
    "            # Label the surrounding words as product\n",
    "            start = max(0, i-3)\n",
    "            end = min(len(tokens), i+4)\n",
    "            labels[start] = 'B-PRODUCT'\n",
    "            for j in range(start+1, end):\n",
    "                labels[j] = 'I-PRODUCT'\n",
    "    \n",
    "    return list(zip(tokens, labels))\n",
    "\n",
    "def save_to_conll(labeled_data, filename):\n",
    "    \"\"\"Save labeled data in CoNLL format\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for message in labeled_data:\n",
    "            for token, label in message:\n",
    "                f.write(f\"{token}\\t{label}\\n\")\n",
    "            f.write(\"\\n\")  # Empty line between messages\n",
    "\n",
    "# Label all sample messages\n",
    "labeled_messages = [label_message(msg) for msg in sample_messages]\n",
    "\n",
    "# Save to CONLL file\n",
    "save_to_conll(labeled_messages, 'labeled_data.conll')\n",
    "print(f\"Successfully labeled {len(labeled_messages)} messages in CoNLL format.\")\n",
    "# Step 1: Install Dependencies\n",
    "%pip install -q transformers datasets seqeval\n",
    "\n",
    "# Step 2: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    ")\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "# Step 3: Load Tokenizer and Model (Choose one)\n",
    "# Use a valid public model for Amharic or multilingual NER\n",
    "# For Amharic NER, you can use: \"Davlan/xlm-roberta-base-finetuned-ner-afr\"\n",
    "# Or fallback to multilingual: \"xlm-roberta-base\"\n",
    "model_checkpoint = \"Davlan/xlm-roberta-base-finetuned-ner-afr\"\n",
    "\n",
    "# Ensure required dependencies are installed\n",
    "%pip install -q transformers sentencepiece protobuf\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Use use_fast=False for compatibility with some multilingual models\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)\n",
    "#model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=5)  # adjust num_labels as needed\n",
    "\n",
    "# Step 4: Load or Parse CoNLL-format Data\n",
    "# using Hugging Face `load_dataset`\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=5)  # adjust num_labels as needed\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)\n",
    "# Function to parse CoNLL file into a Hugging Face Dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "def parse_conll_file(filepath):\n",
    "    sentences, labels = [], []\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        words, tags = [], []\n",
    "        for line in file:\n",
    "            if line.strip() == \"\":\n",
    "                if words:\n",
    "                    sentences.append(words)\n",
    "                    labels.append(tags)\n",
    "                    words, tags = [], []\n",
    "            else:\n",
    "                parts = line.strip().split()\n",
    "                words.append(parts[0])\n",
    "                tags.append(parts[-1])\n",
    "    return Dataset.from_dict({\"tokens\": sentences, \"ner_tags\": labels})\n",
    "\n",
    "dataset = parse_conll_file(\"labeled_data.conll\")  \n",
    "label_list = list(set(tag for row in dataset[\"ner_tags\"] for tag in row))\n",
    "label_list.sort()\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "# Convert labels to ids\n",
    "def encode_labels(example):\n",
    "    example[\"labels\"] = [label2id[tag] for tag in example[\"ner_tags\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(encode_labels)\n",
    "\n",
    "# Load the tokenizer if not already loaded\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)\n",
    "\n",
    "# Step 5: Tokenize and Align Labels\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    word_ids = tokenized.word_ids()\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(example[\"labels\"][word_idx])\n",
    "        else:\n",
    "            label_ids.append(example[\"labels\"][word_idx] if label_list[example[\"labels\"][word_idx]].startswith(\"B-\") else example[\"labels\"][word_idx])\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized[\"labels\"] = label_ids\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=False)\n",
    "\n",
    "# Step 6: Split Train and Validation\n",
    "train_test = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test[\"train\"]\n",
    "eval_dataset = train_test[\"test\"]\n",
    "\n",
    "# Step 7: Set Up Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./amharic-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=2)\n",
    "    labels = pred.label_ids\n",
    "    true_preds = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(preds, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(preds, labels)\n",
    "    ]\n",
    "    return {\"f1\": classification_report(true_labels, true_preds, output_dict=True)[\"weighted avg\"][\"f1\"]}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Step 8: Train Model\n",
    "trainer.train()\n",
    "\n",
    "# Step 9: Save Fine-Tuned Model\n",
    "model.save_pretrained(\"./amharic-ner-model\")\n",
    "tokenizer.save_pretrained(\"./amharic-ner-model\")\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "# Models to Compare\n",
    "models = {\n",
    "    \"xlm-roberta\": \"Davlan/xlm-roberta-base-ner-hrl\",\n",
    "    \"mbert\": \"bert-base-multilingual-cased\",\n",
    "    \"distilbert\": \"Davlan/distilbert-base-multilingual-cased-ner-hrl\"\n",
    "}\n",
    "\n",
    "def train_and_evaluate(model_name, model_checkpoint, train_dataset, eval_dataset, label_list, id2label, label2id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list), id2label=id2label, label2id=label2id)\n",
    "\n",
    "    def tokenize_and_align_labels(example):\n",
    "        tokenized = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "        word_ids = tokenized.word_ids()\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(example[\"labels\"][word_idx])\n",
    "            else:\n",
    "                label_ids.append(example[\"labels\"][word_idx] if label_list[example[\"labels\"][word_idx]].startswith(\"B-\") else example[\"labels\"][word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        tokenized[\"labels\"] = label_ids\n",
    "        return tokenized\n",
    "\n",
    "    tokenized_train = train_dataset.map(tokenize_and_align_labels)\n",
    "    tokenized_eval = eval_dataset.map(tokenize_and_align_labels)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./results_{model_name}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    def compute_metrics(pred):\n",
    "        preds = np.argmax(pred.predictions, axis=2)\n",
    "        labels = pred.label_ids\n",
    "        true_preds = [\n",
    "            [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(preds, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(preds, labels)\n",
    "        ]\n",
    "        return {\"f1\": classification_report(true_labels, true_preds, output_dict=True)[\"weighted avg\"][\"f1\"]}\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_eval,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    results = trainer.evaluate()\n",
    "    return model, tokenizer, results\n",
    "\n",
    "# Example Loop\n",
    "results_dict = {}\n",
    "for name, checkpoint in models.items():\n",
    "    print(f\"\\nTraining model: {name}\")\n",
    "    model, tokenizer, metrics = train_and_evaluate(name, checkpoint, train_dataset, eval_dataset, label_list, id2label, label2id)\n",
    "    results_dict[name] = metrics\n",
    "\n",
    "# Best model based on F1 score\n",
    "best_model_name = max(results_dict, key=lambda x: results_dict[x]['eval_f1'])\n",
    "print(f\"✅ Best Model: {best_model_name} with F1 = {results_dict[best_model_name]['eval_f1']:.4f}\")\n",
    "# Save the best model\n",
    "best_model = models[best_model_name]\n",
    "model.save_pretrained(f\"./best_amharic_ner_model_{best_model_name}\")\n",
    "tokenizer.save_pretrained(f\"./best_amharic_ner_model_{best_model_name}\")\n",
    "!pip install -q shap lime\n",
    "\n",
    "import shap\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# Build a prediction pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Sample Amharic text\n",
    "example_text = \"አዲስ ማርቆስ የዋጋ 600 ብር ጫማ ተሽከርካሪ\"\n",
    "\n",
    "# Use SHAP\n",
    "explainer = shap.Explainer(ner_pipeline)\n",
    "shap_values = explainer([example_text])\n",
    "shap.plots.text(shap_values[0])\n",
    "\n",
    "# Use LIME (optional — works better for classification)\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "lime_explainer = LimeTextExplainer(class_names=label_list)\n",
    "\n",
    "def predict_proba(texts):\n",
    "    preds = []\n",
    "    for text in texts:\n",
    "        outputs = ner_pipeline(text)\n",
    "        proba = [0]*len(label_list)\n",
    "        for o in outputs:\n",
    "            proba[label2id[o['entity_group']]] += 1\n",
    "        preds.append([p/len(outputs) for p in proba])\n",
    "    return np.array(preds)\n",
    "\n",
    "lime_exp = lime_explainer.explain_instance(example_text, predict_proba, num_features=5)\n",
    "lime_exp.show_in_notebook()\n",
    "# Save the SHAP and LIME explanations\n",
    "shap_values.save(\"shap_explanations.pkl\")\n",
    "lime_exp.save_to_file(\"lime_explanations.html\")\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "\n",
    "# Example vendor data format\n",
    "# Each row: {\"vendor_id\": \"Vendor1\", \"text\": \"...\", \"views\": 120, \"timestamp\": \"2024-06-20\"}\n",
    "\n",
    "def analyze_vendor(posts_df, ner_pipeline):\n",
    "    scorecard = []\n",
    "\n",
    "    for vendor_id, group in posts_df.groupby(\"vendor_id\"):\n",
    "        # Posting frequency\n",
    "        group['timestamp'] = pd.to_datetime(group['timestamp'])\n",
    "        weekly_posts = group.set_index('timestamp').resample('W').size().mean()\n",
    "\n",
    "        # Views and top post\n",
    "        avg_views = group['views'].mean()\n",
    "        top_post = group.loc[group['views'].idxmax()]\n",
    "        top_product = ner_pipeline(top_post['text'])\n",
    "\n",
    "        # Extract product prices for average\n",
    "        prices = []\n",
    "        for _, row in group.iterrows():\n",
    "            entities = ner_pipeline(row['text'])\n",
    "            for ent in entities:\n",
    "                if ent['entity_group'] == \"PRICE\":\n",
    "                    try:\n",
    "                        prices.append(float(ent['word'].replace(\"ብር\", \"\").replace(\",\", \"\").strip()))\n",
    "                    except:\n",
    "                        continue\n",
    "        avg_price = mean(prices) if prices else 0\n",
    "\n",
    "        # Final lending score (simple version)\n",
    "        lending_score = (0.5 * avg_views) + (0.5 * weekly_posts)\n",
    "\n",
    "        scorecard.append({\n",
    "            \"vendor_id\": vendor_id,\n",
    "            \"avg_views\": avg_views,\n",
    "            \"posting_frequency\": weekly_posts,\n",
    "            \"avg_price\": avg_price,\n",
    "            \"top_product\": top_product,\n",
    "            \"lending_score\": lending_score\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(scorecard).sort_values(\"lending_score\", ascending=False)\n",
    "\n",
    "# Usage\n",
    "vendor_df = pd.read_csv(\"telegram_posts.csv\") # Load your vendor data\n",
    "score_df = analyze_vendor(vendor_df, ner_pipeline)\n",
    "print(score_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
