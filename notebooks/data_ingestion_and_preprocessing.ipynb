{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8db71c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from pandas) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: telethon in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (1.40.0)\n",
      "Requirement already satisfied: pyaes in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from telethon) (1.6.1)\n",
      "Requirement already satisfied: rsa in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from telethon) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from rsa->telethon) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: Pillow in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (11.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-magic-bin in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (0.4.14)\n",
      "Requirement already satisfied: numpy in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: telethon in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (1.40.0)\n",
      "Requirement already satisfied: pyaes in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from telethon) (1.6.1)\n",
      "Requirement already satisfied: rsa in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from telethon) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\yohanan\\amharic-e-commerce-data-extractor\\.venv\\lib\\site-packages (from rsa->telethon) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Fetching messages from @ZemenExpress...\n",
      "Error accessing channel @ZemenExpress: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_10580\\3607690783.py:43: RuntimeWarning: coroutine 'AuthMethods._start' was never awaited\n",
      "  self.client.start(PHONE_NUMBER)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_10580\\3607690783.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_10580\\3607690783.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_10580\\3607690783.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_10580\\3607690783.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_10580\\3607690783.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_10580\\3607690783.py:43: RuntimeWarning: coroutine 'AuthMethods._start' was never awaited\n",
      "  self.client.start(PHONE_NUMBER)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_10580\\3607690783.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 0 records to processed_data\\@ZemenExpress_messages.csv\n",
      "Fetching messages from @nevacomputer...\n",
      "Error accessing channel @nevacomputer: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@nevacomputer_messages.csv\n",
      "Fetching messages from @meneshayeofficial...\n",
      "Error accessing channel @meneshayeofficial: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@meneshayeofficial_messages.csv\n",
      "Fetching messages from @ethio_brand_collection...\n",
      "Error accessing channel @ethio_brand_collection: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@ethio_brand_collection_messages.csv\n",
      "Fetching messages from @Leyueqa@AwasMart...\n",
      "Error accessing channel @Leyueqa@AwasMart: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@Leyueqa@AwasMart_messages.csv\n",
      "Data collection complete!\n",
      "Fetching messages from @ZemenExpress...\n",
      "Error accessing channel @ZemenExpress: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@ZemenExpress_messages.csv\n",
      "Fetching messages from @nevacomputer...\n",
      "Error accessing channel @nevacomputer: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@nevacomputer_messages.csv\n",
      "Fetching messages from @meneshayeofficial...\n",
      "Error accessing channel @meneshayeofficial: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@meneshayeofficial_messages.csv\n",
      "Fetching messages from @ethio_brand_collection...\n",
      "Error accessing channel @ethio_brand_collection: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@ethio_brand_collection_messages.csv\n",
      "Fetching messages from @Leyueqa@AwasMart...\n",
      "Error accessing channel @Leyueqa@AwasMart: You must use \"async for\" if the event loop is running (i.e. you are inside an \"async def\")\n",
      "Saved 0 records to processed_data\\@Leyueqa@AwasMart_messages.csv\n",
      "Data collection complete!\n",
      "Scraping complete!\n",
      "Warning: 'text' column not found in DataFrame. Returning empty DataFrame.\n",
      "Processed data saved to fully_processed_data.csv\n",
      "Data preprocessing complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_10580\\3607690783.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_10580\\3607690783.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_10580\\3607690783.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\Yohanan\\AppData\\Local\\Temp\\ipykernel_10580\\3607690783.py:228: RuntimeWarning: coroutine 'UserMethods.get_entity' was never awaited\n",
      "  channel_messages = self.fetch_channel_messages(channel)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmptyDataError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 337\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(\u001b[33m'\u001b[39m\u001b[33mfully_processed_data.csv\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m os.path.getsize(\u001b[33m'\u001b[39m\u001b[33mfully_processed_data.csv\u001b[39m\u001b[33m'\u001b[39m) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfully_processed_data.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    339\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWarning: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfully_processed_data.csv\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is empty or does not exist. Creating empty DataFrame.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yohanan\\Amharic-E-commerce-data-extractor\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yohanan\\Amharic-E-commerce-data-extractor\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yohanan\\Amharic-E-commerce-data-extractor\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yohanan\\Amharic-E-commerce-data-extractor\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Yohanan\\Amharic-E-commerce-data-extractor\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:581\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mEmptyDataError\u001b[39m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install telethon \n",
    "%pip install Pillow \n",
    "%pip install python-magic-bin numpy\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "%pip install telethon\n",
    "from telethon import TelegramClient\n",
    "from telethon.tl.types import MessageMediaPhoto, MessageMediaDocument\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import magic\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuration\n",
    "API_ID = '20456758'  # Replace with your API ID\n",
    "API_HASH = '83bb06a8c677ed8128784c2dc575aff6'  # Replace with your API hash\n",
    "PHONE_NUMBER = '+251920747086'  # Replace with your phone number\n",
    "SESSION_NAME = 'amharic_ecommerce_scraper'\n",
    "# List of Ethiopian e-commerce channels to monitor\n",
    "CHANNELS = [\n",
    "    '@ZemenExpress',          \n",
    "    '@nevacomputer',    \n",
    "    '@meneshayeofficial',\n",
    "    '@ethio_brand_collection',       \n",
    "    '@Leyueqa' \n",
    "    '@AwasMart',    \n",
    "]\n",
    "# Output directories\n",
    "RAW_DATA_DIR = 'raw_data'\n",
    "PROCESSED_DATA_DIR = 'processed_data'\n",
    "IMAGES_DIR = os.path.join(RAW_DATA_DIR, 'images')\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "class TelegramScraper:\n",
    "    def __init__(self):\n",
    "        self.client = TelegramClient(SESSION_NAME, API_ID, API_HASH)\n",
    "        self.client.start(PHONE_NUMBER)\n",
    "\n",
    "    def fetch_channel_messages(self, channel, limit=100):\n",
    "        \"\"\"\n",
    "        Fetch messages from a Telegram channel.\n",
    "        Returns a list of dictionaries representing messages.\n",
    "        \"\"\"\n",
    "        messages = []\n",
    "        try:\n",
    "            entity = self.client.get_entity(channel)\n",
    "            for message in self.client.iter_messages(entity, limit=limit):\n",
    "                messages.append({\n",
    "                    'channel': channel,\n",
    "                    'message_id': message.id,\n",
    "                    'date': message.date,\n",
    "                    'text': message.text,\n",
    "                    'views': getattr(message, 'views', None),\n",
    "                    'sender_id': getattr(message, 'sender_id', None)\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching messages from {channel}: {e}\")\n",
    "        return messages\n",
    " \n",
    "    def clean_amharic_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean and normalize Amharic text\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "# Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove emojis and special characters (keeping Amharic characters)\n",
    "        # Amharic Unicode range: U+1200 to U+137F\n",
    "        text = re.sub(r'[^\\u1200-\\u137F\\s.,!?።፣፤፥፦፧፨0-9a-zA-Z]', '', text)\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        \"\"\"\n",
    "        Basic entity extraction for Amharic e-commerce text\n",
    "        \"\"\"\n",
    "        entities = {\n",
    "            'prices': [],\n",
    "            'products': [],\n",
    "            'locations': [],\n",
    "            'contacts': []\n",
    "        }\n",
    "        \n",
    "        if not text:\n",
    "            return entities\n",
    "        \n",
    "        # Extract prices (numbers with currency symbols or words)\n",
    "        price_patterns = [\n",
    "            r'(\\d+)\\s*(ብር|br|birr|BR|ብሮ|ብሬ)',  # Ethiopian Birr\n",
    "            r'\\$\\s*(\\d+)',                      # US Dollars\n",
    "            r'(\\d+)\\s*(ዶላር|dollar)'             # Dollars in Amharic\n",
    "        ]\n",
    "        \n",
    "        for pattern in price_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            for match in matches:\n",
    "                if match[0].isdigit():\n",
    "                    entities['prices'].append(match[0] + ' ' + (match[1] if len(match) > 1 else 'ብር'))\n",
    "        \n",
    "        # Extract phone numbers (Ethiopian format)\n",
    "        phone_matches = re.findall(r'(?:\\+251|0)(?:9\\d{8}|[1-9]\\d{7})', text)\n",
    "        entities['contacts'].extend(phone_matches)\n",
    "        \n",
    "        # Extract locations (common Ethiopian cities/areas)\n",
    "        locations = ['አዲስ አበባ', 'ባህር �ር', 'ድሬ ዳዋ', 'ጅማ', 'መቀሌ', \n",
    "                    'አዋሳ', 'አርባ ምንጭ', 'አዲስ አበባ', 'ባህር ዳር', 'ጎንደር']\n",
    "        entities['locations'] = [loc for loc in locations if loc in text]\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def download_media(self, message):\n",
    "        \"\"\"\n",
    "        Download and process media files (images, documents)\n",
    "        \"\"\"\n",
    "        media_info = {\n",
    "            'media_type': None,\n",
    "            'file_path': None,\n",
    "            'file_size': None,\n",
    "            'dimensions': None\n",
    "        }\n",
    "        \n",
    "        if not message.media:\n",
    "            return media_info\n",
    "        \n",
    "        try:\n",
    "            if isinstance(message.media, MessageMediaPhoto):\n",
    "                # Download photo\n",
    "                media_info['media_type'] = 'photo'\n",
    "                file_path = os.path.join(IMAGES_DIR, f'photo_{message.id}.jpg')\n",
    "                self.client.download_media(message.media, file=file_path)\n",
    "                \n",
    "                # Get image dimensions\n",
    "                with Image.open(file_path) as img:\n",
    "                    media_info['dimensions'] = f\"{img.width}x{img.height}\"\n",
    "                \n",
    "                media_info['file_path'] = file_path\n",
    "                media_info['file_size'] = os.path.getsize(file_path)\n",
    "                \n",
    "            elif isinstance(message.media, MessageMediaDocument):\n",
    "                # Download document\n",
    "                media_info['media_type'] = 'document'\n",
    "                file_name = f'document_{message.id}'\n",
    "                file_path = os.path.join(RAW_DATA_DIR, file_name)\n",
    "                self.client.download_media(message.media, file=file_path)\n",
    "                \n",
    "                # Get file type\n",
    "                mime = magic.Magic(mime=True)\n",
    "                file_type = mime.from_file(file_path)\n",
    "                \n",
    "                # Rename file with proper extension\n",
    "                ext = file_type.split('/')[-1]\n",
    "                new_path = f\"{file_path}.{ext}\"\n",
    "                os.rename(file_path, new_path)\n",
    "                \n",
    "                media_info['file_path'] = new_path\n",
    "                media_info['file_size'] = os.path.getsize(new_path)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading media: {e}\")\n",
    "        \n",
    "        return media_info\n",
    "    \n",
    "    def fetch_channel_messages(self, channel_name, limit=1000):\n",
    "        \"\"\"\n",
    "        Fetch messages from a Telegram channel\n",
    "        \"\"\"\n",
    "        print(f\"Fetching messages from {channel_name}...\")\n",
    "        \n",
    "        messages_data = []\n",
    "        try:\n",
    "            channel = self.client.get_entity(channel_name)\n",
    "            \n",
    "            for message in self.client.iter_messages(channel, limit=limit):\n",
    "                try:\n",
    "                    # Basic message info\n",
    "                    msg_data = {\n",
    "                        'channel': channel_name,\n",
    "                        'message_id': message.id,\n",
    "                        'date': message.date,\n",
    "                        'text': self.clean_amharic_text(message.text),\n",
    "                        'views': message.views if hasattr(message, 'views') else None,\n",
    "                        'sender': message.sender_id if hasattr(message, 'sender_id') else None\n",
    "                    }\n",
    "                    \n",
    "                    # Download and process media\n",
    "                    media_info = self.download_media(message)\n",
    "                    msg_data.update(media_info)\n",
    "                    \n",
    "                    # Extract entities from text\n",
    "                    entities = self.extract_entities(message.text)\n",
    "                    msg_data.update(entities)\n",
    "                    \n",
    "                    messages_data.append(msg_data)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing message {message.id}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing channel {channel_name}: {e}\")\n",
    "        \n",
    "        return messages_data\n",
    "    \n",
    "    def save_to_csv(self, data, filename):\n",
    "        \"\"\"Save data to CSV file\"\"\"\n",
    "        df = pd.DataFrame(data)\n",
    "        filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "        df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "        print(f\"Saved {len(df)} records to {filepath}\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Main execution method\"\"\"\n",
    "        all_messages = []\n",
    "        \n",
    "        # Fetch messages from all channels\n",
    "        for channel in CHANNELS:\n",
    "            channel_messages = self.fetch_channel_messages(channel)\n",
    "            all_messages.extend(channel_messages)\n",
    "            \n",
    "            # Save individual channel data\n",
    "            self.save_to_csv(channel_messages, f'{channel}_messages.csv')\n",
    "        \n",
    "        # Save combined data\n",
    "        if all_messages:\n",
    "            self.save_to_csv(all_messages, 'all_channels_combined.csv')\n",
    "        \n",
    "        print(\"Data collection complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = TelegramScraper()\n",
    "    scraper.run()\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load all CSV files from the data directory, skipping empty files\"\"\"\n",
    "        all_data = []\n",
    "        for file in os.listdir(self.data_dir):\n",
    "            if file.endswith('.csv'):\n",
    "                filepath = os.path.join(self.data_dir, file)\n",
    "                try:\n",
    "                    # Check if file is not empty before reading\n",
    "                    if os.path.getsize(filepath) == 0:\n",
    "                        print(f\"Skipped empty file: {filepath}\")\n",
    "                        continue\n",
    "                    df = pd.read_csv(filepath, encoding='utf-8')\n",
    "                    if not df.empty:\n",
    "                        all_data.append(df)\n",
    "                except pd.errors.EmptyDataError:\n",
    "                    print(f\"Skipped empty file: {filepath}\")\n",
    "        \n",
    "        if all_data:\n",
    "            return pd.concat(all_data, ignore_index=True)\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Advanced text preprocessing for Amharic\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove repeated characters (common in informal text)\n",
    "        text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "        \n",
    "        # Normalize Amharic numbers to Western numerals\n",
    "        amharic_numerals = {\n",
    "            '፩': '1', '፪': '2', '፫': '3', '፬': '4', '፭': '5',\n",
    "            '፮': '6', '፯': '7', '፰': '8', '፱': '9', '፲': '10'\n",
    "        }\n",
    "        \n",
    "        for am_num, num in amharic_numerals.items():\n",
    "            text = text.replace(am_num, num)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def process_data(self, df):\n",
    "        \"\"\"\n",
    "        Process the raw dataframe\n",
    "        \"\"\"\n",
    "        # Check if 'text' column exists\n",
    "        if 'text' not in df.columns:\n",
    "            print(\"Warning: 'text' column not found in DataFrame. Returning empty DataFrame.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Clean text\n",
    "        df['clean_text'] = df['text'].apply(self.preprocess_text)\n",
    "        \n",
    "        # Convert date to datetime if 'date' column exists\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        \n",
    "        # Extract additional features\n",
    "        df['text_length'] = df['clean_text'].apply(len)\n",
    "        df['word_count'] = df['clean_text'].apply(lambda x: len(x.split()))\n",
    "        \n",
    "        # Handle missing values for expected columns\n",
    "        for col in ['prices', 'locations', 'contacts']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna('[]')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def save_processed_data(self, df, output_file):\n",
    "        \"\"\"Save processed data to file\"\"\"\n",
    "        df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        print(f\"Processed data saved to {output_file}\")\n",
    "# ...existing code...\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = TelegramScraper()\n",
    "    scraper.run()\n",
    "    print(\"Scraping complete!\")\n",
    "    preprocessor = DataPreprocessor(RAW_DATA_DIR)  # <-- use RAW_DATA_DIR here\n",
    "    raw_data = preprocessor.load_data()\n",
    "    processed_data = preprocessor.process_data(raw_data)\n",
    "    preprocessor.save_processed_data(processed_data, 'fully_processed_data.csv')\n",
    "    print(\"Data preprocessing complete!\")\n",
    "    \n",
    "    import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the dataset (assuming we're using the processed data from Task 1)\n",
    "import os\n",
    "if os.path.exists('fully_processed_data.csv') and os.path.getsize('fully_processed_data.csv') > 0:\n",
    "    df = pd.read_csv('fully_processed_data.csv', encoding='utf-8')\n",
    "else:\n",
    "    print(\"Warning: 'fully_processed_data.csv' is empty or does not exist. Creating empty DataFrame.\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# Select 50 random messages for labeling\n",
    "sample_messages = df['text'].dropna().sample(50, random_state=42).tolist()\n",
    "\n",
    "def tokenize_amharic(text):\n",
    "    \"\"\"Tokenize Amharic text (simple whitespace tokenizer with some special handling)\"\"\"\n",
    "    # Split on whitespace but keep common punctuation attached to words\n",
    "    tokens = re.findall(r'\\w+[\\']?\\w*|[\\w\\.-]+|[\\u1200-\\u137F]+|[^\\w\\s]', text)\n",
    "    return [token for token in tokens if token.strip()]\n",
    "\n",
    "def label_message(message):\n",
    "    \"\"\"Label entities in a single message according to CoNLL format\"\"\"\n",
    "    tokens = tokenize_amharic(message)\n",
    "    labels = ['O'] * len(tokens)  # Initialize all tokens as 'O'\n",
    "    \n",
    "    # Patterns for entity recognition\n",
    "    price_pattern = re.compile(r'(\\d+)\\s*(ብር|br|birr|BR|ብሮ|ብሬ|ዶላር|dollar)')\n",
    "    location_words = {'አዲስ አበባ', 'ባህር ዳር', 'ድሬ ዳዋ', 'ጅማ', 'መቀሌ', \n",
    "                     'አዋሳ', 'አርባ ምንጭ', 'ጎንደር', 'ቦሌ', 'ፒያሳ', \n",
    "                     'ሰማንያ', 'አቃቂ', 'ለሚ ካምፓ', 'ሰበር', 'መኮንን'}\n",
    "    \n",
    "    # Convert tokens back to string for pattern matching\n",
    "    text_for_matching = ' '.join(tokens)\n",
    "    \n",
    "    # Label prices\n",
    "    for match in price_pattern.finditer(text_for_matching):\n",
    "        price_start, price_end = match.span()\n",
    "        price_tokens = []\n",
    "        current_pos = 0\n",
    "        for i, token in enumerate(tokens):\n",
    "            token_start = current_pos\n",
    "            token_end = token_start + len(token)\n",
    "            current_pos += len(token) + 1  # +1 for the space\n",
    "            \n",
    "            # Check if token is within the matched price\n",
    "            if not (token_end <= price_start or token_start >= price_end):\n",
    "                price_tokens.append(i)\n",
    "        \n",
    "        if price_tokens:\n",
    "            labels[price_tokens[0]] = 'B-PRICE'\n",
    "            for i in price_tokens[1:]:\n",
    "                labels[i] = 'I-PRICE'\n",
    "    \n",
    "    # Label locations (simple approach)\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in location_words:\n",
    "            labels[i] = 'B-LOC'\n",
    "    \n",
    "    # Label products (this would normally require more sophisticated approach)\n",
    "    # For demo, we'll look for common product indicators\n",
    "    product_keywords = {\n",
    "        'ለገበያ': 'B-PRODUCT',\n",
    "        'ሽያጭ': 'B-PRODUCT', \n",
    "        'ይገኛል': 'B-PRODUCT',\n",
    "        'መሸጥ': 'B-PRODUCT',\n",
    "        'መግዛት': 'B-PRODUCT'\n",
    "    }\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in product_keywords:\n",
    "            # Label the surrounding words as product\n",
    "            start = max(0, i-3)\n",
    "            end = min(len(tokens), i+4)\n",
    "            labels[start] = 'B-PRODUCT'\n",
    "            for j in range(start+1, end):\n",
    "                labels[j] = 'I-PRODUCT'\n",
    "    \n",
    "    return list(zip(tokens, labels))\n",
    "\n",
    "def save_to_conll(labeled_data, filename):\n",
    "    \"\"\"Save labeled data in CoNLL format\"\"\"\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for message in labeled_data:\n",
    "            for token, label in message:\n",
    "                f.write(f\"{token}\\t{label}\\n\")\n",
    "            f.write(\"\\n\")  # Empty line between messages\n",
    "\n",
    "# Label all sample messages\n",
    "labeled_messages = [label_message(msg) for msg in sample_messages]\n",
    "\n",
    "# Save to CONLL file\n",
    "save_to_conll(labeled_messages, 'labeled_data.conll')\n",
    "print(f\"Successfully labeled {len(labeled_messages)} messages in CoNLL format.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
